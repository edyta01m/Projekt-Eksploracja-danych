) %>%
lapply(as.numeric)
mae <- with(reduction, total_abs_error / total_samples_seen)
return(mae)
}
# Ocena na zbiorze walidacyjnym
val_mae <- evaluate_naive_method(val_dataset)
library(tfdatasets)
# Funkcja oceny modelu heurystycznego
evaluate_naive_method <- function(dataset) {
unnormalize_gold_price <- function(x) {
(x * normalization_values$sd) + normalization_values$mean
}
reduction <- dataset %>%
dataset_unbatch() %>%
dataset_map(function(samples, target) {
last_price_in_input <- samples[-1, ]
pred <- unnormalize_gold_price(last_price_in_input)
abs(pred - target)
}) %>%
dataset_reduce(
initial_state = list(total_samples_seen = 0L, total_abs_error = 0),
reduce_func = function(state, element) {
state$total_samples_seen %<>% `+`(1L)
state$total_abs_error %<>% `+`(element)
state
}
) %>%
lapply(as.numeric)
mae <- with(reduction, total_abs_error / total_samples_seen)
return(mae)
}
# Ocena na zbiorze walidacyjnym
val_mae <- evaluate_naive_method(val_dataset)
sprintf("Validation MAE: %.2f", val_mae)
library(tfdatasets)
# Funkcja oceniająca metodę heurystyczną
evaluate_naive_method <- function(dataset) {
unnormalize_gold_price <- function(x) {
(x * normalization_values$sd) + normalization_values$mean
}
total_abs_error <- tf$Variable(0.0, dtype=tf$float64)
total_samples_seen <- tf$Variable(0L, dtype=tf$int64)
it <- iter_dataset(dataset)
while(TRUE) {
batch <- next_batch(it, batch_size=batch_size)
if (is.null(batch)) break
samples <- batch[['samples']][[1]]
targets <- batch[['targets']][[1]]
last_price_in_input <- samples[nrow(samples), ]
pred <- unnormalize_gold_price(last_price_in_input)
abs_error <- tf$abs(pred - targets)
total_abs_error$assign_add(tf$reduce_sum(abs_error))
total_samples_seen$assign_add(tf$cast(tf$shape(targets)[1], tf$int64))
}
mae <- total_abs_error / tf$cast(total_samples_seen, tf$float64)
# Sprawdzenie czy wynik nie jest NaN
if (tf$is_nan(mae)) {
cat("Warning: NaN encountered in MAE calculation.\n")
mae <- tf$constant(0.0, dtype=tf$float64)  # lub inna obsługa NaN
}
return(mae)
}
# Wywołanie funkcji na zbiorze walidacyjnym
val_mae <- evaluate_naive_method(val_dataset)
library(tfdatasets)
# Funkcja oceniająca metodę heurystyczną
evaluate_naive_method <- function(dataset) {
unnormalize_gold_price <- function(x) {
(x * normalization_values$sd) + normalization_values$mean
}
total_abs_error <- tf$Variable(0.0, dtype=tf$float64)
total_samples_seen <- tf$Variable(0L, dtype=tf$int64)
# Iteracja przez zbiór danych
dataset %>% foreach_batch({
function(samples, targets) {
last_price_in_input <- samples[nrow(samples), ]
pred <- unnormalize_gold_price(last_price_in_input)
abs_error <- tf$abs(pred - targets)
total_abs_error$assign_add(tf$reduce_sum(abs_error))
total_samples_seen$assign_add(tf$cast(tf$shape(targets)[1], tf$int64))
}
})
mae <- total_abs_error / tf$cast(total_samples_seen, tf$float64)
# Sprawdzenie czy wynik nie jest NaN
if (tf$is_nan(mae)) {
cat("Warning: NaN encountered in MAE calculation.\n")
mae <- tf$constant(0.0, dtype=tf$float64)  # lub inna obsługa NaN
}
return(mae)
}
# Wywołanie funkcji na zbiorze walidacyjnym
val_mae <- evaluate_naive_method(val_dataset)
library(tfdatasets)
# Funkcja oceniająca metodę heurystyczną
evaluate_naive_method <- function(dataset) {
unnormalize_gold_price <- function(x) {
(x * normalization_values$sd) + normalization_values$mean
}
total_abs_error <- tf$Variable(0.0, dtype=tf$float64)
total_samples_seen <- tf$Variable(0L, dtype=tf$int64)
# Mapowanie datasetu
dataset %>%
dataset_map(function(samples, targets) {
last_price_in_input <- samples[nrow(samples), ]
pred <- unnormalize_gold_price(last_price_in_input)
abs_error <- tf$abs(pred - targets)
total_abs_error$assign_add(tf$reduce_sum(abs_error))
total_samples_seen$assign_add(tf$cast(tf$shape(targets)[1], tf$int64))
return(targets)  # Zwróć targets, aby poprawnie przekazać dataset dalej
})
mae <- total_abs_error / tf$cast(total_samples_seen, tf$float64)
# Sprawdzenie czy wynik nie jest NaN
if (tf$is_nan(mae)) {
cat("Warning: NaN encountered in MAE calculation.\n")
mae <- tf$constant(0.0, dtype=tf$float64)  # lub inna obsługa NaN
}
return(mae)
}
# Wywołanie funkcji na zbiorze walidacyjnym
val_mae <- evaluate_naive_method(val_dataset)
# Wyświetlenie wyniku MAE
sprintf("Validation MAE: %.2f", as.numeric(val_mae))
any(is.na(val_dataset))
any(is.infinite(val_dataset))
has_nan <- any(tf$math$reduce_any(tf$is_nan(val_dataset)))
has_nan <- any(tf$math$reduce_any(tf$is_nan(val_dataset)))
View(val)
View(train)
View(val)
View(test)
# Ocena na zbiorze testowym
test_mae <- evaluate_naive_method(test_dataset)
sprintf("Test MAE: %.2f", test_mae)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(rio)
data <- import(file ="gold_price_data.csv")
library(rio)
data <- import(file ="gold_price_data.csv")
data <- import(file ="gold_price_data.csv")
head(data)
library(rio)
data <- import(file ="gold_price_data.csv")
library(rio)
data <- import("gold_price_data.csv")
library(rio)
data <- import("D:\\Desktop\\studia2\\projekt_ZMUM_WMUM\\gold_price_data.csv")
data <- read.csv("D:\\Desktop\\studia2\\projekt_ZMUM_WMUM\\gold_price_data.csv")
#sprawdzanie brakow danych
sum(is.na(data))
#sprawdzenie duplikatow danych
sum(duplicated(data))
#przeksztalcenie daty do typu DateTime
data$Date <- as.Date(data$Date)
summary(data)
plot(Value ~ Date, data = data, pch = 20, cex = .3)
library(dplyr)
library(tidyr)
df <- data %>%
group_by(Date) %>%
summarise(Value = mean(Value)) %>%
complete(Date = seq.Date(min(Date), max(Date), by = "day")) %>%
fill(Value)
plot(density(data$Value))
#ogolny podzial danych
num_train_samples <- round(nrow(df) * .5)
num_val_samples <- round(nrow(df) * 0.25)
num_test_samples <- nrow(df) - num_train_samples - num_val_samples
train <- df[seq(num_train_samples), ]
val <- df[seq(from = nrow(df) + 1,
length.out = num_val_samples), ]
test <- df[seq(to = nrow(df),
length.out = num_test_samples), ]
View(val)
View(test)
View(train)
View(val)
#ogolny podzial danych
# Obliczanie liczby próbek dla każdego zbioru
num_samples <- nrow(df)
num_train_samples <- round(num_samples * 0.5)
num_val_samples <- round(num_samples * 0.25)
num_test_samples <- num_samples - num_train_samples - num_val_samples
# Indeksy dla zbioru treningowego
train_indices <- seq_len(num_train_samples)
# Indeksy dla zbioru walidacyjnego
val_indices <- seq(from = num_train_samples + 1,
length.out = num_val_samples)
# Indeksy dla zbioru testowego
test_indices <- seq(from = num_train_samples + num_val_samples + 1,
length.out = num_test_samples)
# Podział danych na zbiory
train <- df[train_indices, ]
val <- df[val_indices, ]
test <- df[test_indices, ]
View(val)
View(train)
View(test)
# Obliczanie średniej i odchylenia standardowego dla kolumny 'Value' w zbiorze treningowym
normalization_values <- list(
mean = mean(train$Value),
sd = sd(train$Value)
)
#definiowanie funkcji normalizujacej
normalize_column <- function(df, column_name, mean_value, sd_value) {
df[[column_name]] <- (df[[column_name]] - mean_value) / sd_value
return(df)
}
#normalizacja dla wszystkich zbiorów
train <- normalize_column(train, "Value", normalization_values$mean, normalization_values$sd)
val <- normalize_column(val, "Value", normalization_values$mean, normalization_values$sd)
test <- normalize_column(test, "Value", normalization_values$mean, normalization_values$sd)
library(keras)
library(tensorflow)
#podzial danych danych do trenowania sieci neuronowej z uwzględnieniem specyfiki szeregów czasowych
sampling_rate <- 1
sequence_length <- 120
delay <- 30
batch_size <- 32 #liczbę próbek danych przetwarzanych przed zaktualizowaniem parametrów modelu. Innymi słowy, batch size to liczba próbek danych używanych do obliczenia pojedynczego kroku gradientu
# Funkcja tworząca zbiór danych do modelowania
make_dataset <- function(data) {
dataset <- timeseries_dataset_from_array(
data = as.matrix(data[, "Value"]),
targets = as.matrix(data[, "Value"]),
sampling_rate = sampling_rate,
sequence_length = sequence_length,
shuffle = TRUE,
batch_size = batch_size
)
return(dataset)
}
train_dataset <- make_dataset(train)
val_dataset <- make_dataset(val)
test_dataset <- make_dataset(test)
library(tfdatasets)
# Funkcja oceny modelu heurystycznego
evaluate_naive_method <- function(dataset) {
unnormalize_gold_price <- function(x) {
(x * normalization_values$sd) + normalization_values$mean
}
reduction <- dataset %>%
dataset_unbatch() %>%
dataset_map(function(samples, target) {
last_price_in_input <- samples[-1, ]
pred <- unnormalize_gold_price(last_price_in_input)
abs(pred - target)
}) %>%
dataset_reduce(
initial_state = list(total_samples_seen = 0L, total_abs_error = 0),
reduce_func = function(state, element) {
state$total_samples_seen %<>% `+`(1L)
state$total_abs_error %<>% `+`(element)
state
}
) %>%
lapply(as.numeric)
mae <- with(reduction, total_abs_error / total_samples_seen)
return(mae)
}
# Ocena na zbiorze walidacyjnym
val_mae <- evaluate_naive_method(val_dataset)
sprintf("Validation MAE: %.2f", val_mae)
# Ocena na zbiorze testowym
test_mae <- evaluate_naive_method(test_dataset)
sprintf("Test MAE: %.2f", test_mae)
# Ocena na zbiorze testowym
test_mae <- evaluate_naive_method(test_dataset)
sprintf("Wartość MAE na zbiorze testowym: %.2f", test_mae)
sprintf("Wartość MAE na zbiorze walidacyjnym: %.2f", val_mae)
View(data)
#Definicja modelu: Tworzymy sekwencyjny model za pomocą keras_model_sequential(), co oznacza, że warstwy są dodawane sekwencyjnie.
model <- keras_model_sequential() %>%
layer_dense(units = 16, activation = "relu", input_shape = ncol_input_data) %>%
layer_dense(units = 1)
input_shape <- 1 #liczba kolumn wejsciowych
#Definicja modelu: Tworzymy sekwencyjny model za pomocą keras_model_sequential(), co oznacza, że warstwy są dodawane sekwencyjnie.
model <- keras_model_sequential() %>%
layer_dense(units = 16, activation = "relu", input_shape = ncol_input_data) %>%
layer_dense(units = 1)
#Definicja modelu: Tworzymy sekwencyjny model za pomocą keras_model_sequential(), co oznacza, że warstwy są dodawane sekwencyjnie.
model <- keras_model_sequential() %>%
layer_dense(units = 16, activation = "relu", input_shape = 1) %>%
layer_dense(units = 1)
#layer_dense(units = 16, activation = "relu", input_shape = ncol_input_data): Pierwsza warstwa gęsta z 16 neuronami, funkcją aktywacji ReLU (Rectified Linear Unit) i kształtem wejściowym odpowiadającym liczbie kolumn danych wejściowych (ncol_input_data).
#layer_dense(units = 1): Ostatnia warstwa gęsta z jednym neuronem, co jest typowe dla problemów regresji, gdzie przewidujemy pojedynczą wartość wyjściową.
#Kompilacja modelu: Kompilujemy model, ustawiając optymalizator rmsprop, funkcję straty mse (Mean Squared Error) i metrykę mae (Mean Absolute Error).
model %>%
compile(optimizer = "rmsprop",
loss = "mse",
metrics = "mae")
# Definicja sekwencyjnego modelu
model <- keras_model_sequential()
# Dodanie warstw do modelu
model %>%
layer_dense(units = 16, activation = "relu", input_shape = 1) %>%
layer_dense(units = 1)
# Podsumowanie modelu
summary(model)
# Kompilacja modelu
model %>%
compile(optimizer = "rmsprop",
loss = "mse",
metrics = "mae")
library(keras)
# Definicja sekwencyjnego modelu
model <- keras_model_sequential()
# Dodanie pierwszej warstwy gęstej
model <- model %>%
layer_dense(units = 16, activation = "relu", input_shape = 1)
# Dodanie drugiej warstwy gęstej
model <- model %>%
layer_dense(units = 1)
# Podsumowanie modelu
summary(model)
# Kompilacja modelu
model <- compile(model,
optimizer = "rmsprop",
loss = "mse",
metrics = "mae")
#definicja warstw modelu
inputs <- layer_input(shape = c(sequence_length, 1))
#definicja warstw modelu
outputs <- inputs %>%
layer_flatten() %>%
layer_dense(16, activation="relu") %>%
layer_dense(1)
#kompilacja modelu
model <- keras_model(inputs, outputs)
model %>%
compile(optimizer = "rmsprop",
loss = "mse",
metrics = "mae")
# Definicja warstw modelu
inputs <- layer_input(shape = c(sequence_length, 1))
outputs <- inputs %>%
layer_flatten() %>%
layer_dense(16, activation="relu") %>%
layer_dense(1)
# Kompilacja modelu
model <- keras_model(inputs, outputs) %>%
compile(optimizer = "rmsprop",
loss = "mse",
metrics = "mae")
# Definicja warstw modelu
inputs <- layer_input(shape = c(sequence_length, 1))
outputs <- inputs %>%
layer_flatten() %>%
layer_dense(16, activation="relu") %>%
layer_dense(1)
# Tworzenie modelu funkcjonalnego
model <- keras_model(inputs, outputs)
# Kompilacja modelu
model %>%
compile(optimizer = "rmsprop",
loss = "mse",
metrics = list("mae"))
install.packages("keras")
install.packages("keras")
detach("package:keras", unload = TRUE)
install.packages("keras")
install.packages("keras")
library(keras)
detach("package:reticulate", unload = TRUE)
install.packages("reticulate")
install.packages("reticulate")
library(reticulate)
# Definicja warstw modelu
inputs <- layer_input(shape = c(sequence_length, 1))
outputs <- inputs %>%
layer_flatten() %>%
layer_dense(16, activation="relu") %>%
layer_dense(1)
# Tworzenie modelu funkcjonalnego
model <- keras_model(inputs, outputs)
# Kompilacja modelu
model %>%
compile(optimizer = "rmsprop",
loss = "mse",
metrics = list("mae"))
# Definicja warstw modelu
inputs <- layer_input(shape = c(sequence_length, 1))
outputs <- inputs %>%
layer_flatten() %>%
layer_dense(16, activation="relu") %>%
layer_dense(1)
# Tworzenie modelu funkcjonalnego
model <- keras_model(inputs, outputs)
# Kompilacja modelu
model %>%
compile(optimizer = "rmsprop",
loss = "mse",
metrics = list("mae"))
cat("Keras version: ", keras::keras_version(), "\n")
keras::install_keras()
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
data <- read.csv("D:\\Desktop\\studia2\\projekt_ZMUM_WMUM\\gold_price_data.csv")
head(data)
#sprawdzanie brakow danych
sum(is.na(data))
#sprawdzenie duplikatow danych
sum(duplicated(data))
#przeksztalcenie daty do typu DateTime
data$Date <- as.Date(data$Date)
summary(data)
plot(Value ~ Date, data = data, pch = 20, cex = .3)
library(dplyr)
library(tidyr)
df <- data %>%
group_by(Date) %>%
summarise(Value = mean(Value)) %>%
complete(Date = seq.Date(min(Date), max(Date), by = "day")) %>%
fill(Value)
plot(density(data$Value))
#ogolny podzial danych
# Obliczanie liczby próbek dla każdego zbioru
num_samples <- nrow(df)
num_train_samples <- round(num_samples * 0.5)
num_val_samples <- round(num_samples * 0.25)
num_test_samples <- num_samples - num_train_samples - num_val_samples
# Indeksy dla zbioru treningowego
train_indices <- seq_len(num_train_samples)
# Indeksy dla zbioru walidacyjnego
val_indices <- seq(from = num_train_samples + 1,
length.out = num_val_samples)
# Indeksy dla zbioru testowego
test_indices <- seq(from = num_train_samples + num_val_samples + 1,
length.out = num_test_samples)
# Podział danych na zbiory
train <- df[train_indices, ]
val <- df[val_indices, ]
test <- df[test_indices, ]
# Obliczanie średniej i odchylenia standardowego dla kolumny 'Value' w zbiorze treningowym
normalization_values <- list(
mean = mean(train$Value),
sd = sd(train$Value)
)
#definiowanie funkcji normalizujacej
normalize_column <- function(df, column_name, mean_value, sd_value) {
df[[column_name]] <- (df[[column_name]] - mean_value) / sd_value
return(df)
}
#normalizacja dla wszystkich zbiorów
train <- normalize_column(train, "Value", normalization_values$mean, normalization_values$sd)
val <- normalize_column(val, "Value", normalization_values$mean, normalization_values$sd)
test <- normalize_column(test, "Value", normalization_values$mean, normalization_values$sd)
library(keras)
library(tensorflow)
#podzial danych danych do trenowania sieci neuronowej z uwzględnieniem specyfiki szeregów czasowych
sampling_rate <- 1
sequence_length <- 120
delay <- 30
batch_size <- 32 #liczbę próbek danych przetwarzanych przed zaktualizowaniem parametrów modelu. Innymi słowy, batch size to liczba próbek danych używanych do obliczenia pojedynczego kroku gradientu
# Funkcja tworząca zbiór danych do modelowania
make_dataset <- function(data) {
dataset <- timeseries_dataset_from_array(
data = as.matrix(data[, "Value"]),
targets = as.matrix(data[, "Value"]),
sampling_rate = sampling_rate,
sequence_length = sequence_length,
shuffle = TRUE,
batch_size = batch_size
)
return(dataset)
}
train_dataset <- make_dataset(train)
val_dataset <- make_dataset(val)
test_dataset <- make_dataset(test)
library(tfdatasets)
# Funkcja oceny modelu heurystycznego
evaluate_naive_method <- function(dataset) {
unnormalize_gold_price <- function(x) {
(x * normalization_values$sd) + normalization_values$mean
}
reduction <- dataset %>%
dataset_unbatch() %>%
dataset_map(function(samples, target) {
last_price_in_input <- samples[-1, ]
pred <- unnormalize_gold_price(last_price_in_input)
abs(pred - target)
}) %>%
dataset_reduce(
initial_state = list(total_samples_seen = 0L, total_abs_error = 0),
reduce_func = function(state, element) {
state$total_samples_seen %<>% `+`(1L)
state$total_abs_error %<>% `+`(element)
state
}
) %>%
lapply(as.numeric)
mae <- with(reduction, total_abs_error / total_samples_seen)
return(mae)
}
# Ocena na zbiorze walidacyjnym
val_mae <- evaluate_naive_method(val_dataset)
sprintf("Wartość MAE na zbiorze walidacyjnym: %.2f", val_mae)
# Ocena na zbiorze testowym
test_mae <- evaluate_naive_method(test_dataset)
sprintf("Wartość MAE na zbiorze testowym: %.2f", test_mae)
# Definicja warstw modelu
inputs <- layer_input(shape = c(sequence_length, 1))
outputs <- inputs %>%
layer_flatten() %>%
layer_dense(16, activation="relu") %>%
layer_dense(1)
# Tworzenie modelu funkcjonalnego
model <- keras_model(inputs, outputs)
# Kompilacja modelu
model %>%
compile(optimizer = "rmsprop",
loss = "mse",
metrics = list("mae"))
install.packages("ggplot2")
